# Gu√≠a de Despliegue en Supercomputo (HPC)

Esta gu√≠a proporciona instrucciones completas para desplegar y ejecutar c√°lculos DFT de preconvergencia GaAs en entornos de supercomputo usando SLURM y Singularity.

## üìã Requisitos Previos

### En el Sistema Local
- Docker instalado y configurado
- Git para control de versiones
- Python 3.10+ con dependencias del proyecto

### En el Supercomputo
- SLURM como gestor de colas
- Singularity/Apptainer instalado
- Acceso a nodos de c√≥mputo con GPUs (recomendado)
- Almacenamiento compartido (/scratch, /home)

## üê≥ Construcci√≥n del Contenedor Singularity

### 1. Construir Imagen Singularity

```bash
# En el sistema local o en el cluster (si permite Docker)
sudo docker build -t preconvergencia-gaas:latest .

# Convertir a Singularity (requiere Singularity instalado)
sudo docker run -d --name temp_container preconvergencia-gaas:latest tail -f /dev/null
sudo docker export temp_container | singularity build preconvergencia-gaas.sif docker-import://stdin
sudo docker rm temp_container
```

### 2. Construir Directamente con Singularity

```bash
# Copiar archivos del proyecto al cluster
scp -r . user@cluster:/path/to/project/

# En el cluster, construir la imagen
singularity build preconvergencia-gaas.sif Singularity.def
```

## üîß Configuraci√≥n del Entorno HPC

### Variables de Entorno Recomendadas

```bash
# En ~/.bashrc o en el script de SLURM
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OPENBLAS_NUM_THREADS=1
export MKL_NUM_THREADS=1
export PYSCF_MAX_MEMORY=32000  # MB por tarea
```

### Configuraci√≥n Personalizada

```bash
# Crear archivo de configuraci√≥n HPC
python hpc_config.py

# O configurar manualmente
cat > hpc_config.json << EOF
{
  "omp_num_threads": 8,
  "pyscf_max_memory": 32000,
  "slurm_partition": "gpu",
  "slurm_time": "24:00:00",
  "singularity_image": "preconvergencia-gaas.sif"
}
EOF
```

## üöÄ Ejecuci√≥n en SLURM

### Trabajo Individual

```bash
# Ejecutar trabajo b√°sico
sbatch slurm_job.sh

# Ver estado del trabajo
squeue -u $USER

# Ver salida del trabajo
tail -f slurm-*.out
```

### Trabajo de Array (M√∫ltiples Configuraciones)

```bash
# Ejecutar array de trabajos
sbatch slurm_array_job.sh

# Monitorear progreso
squeue -u $USER --array
```

### Trabajo Multi-Nodo

```bash
# Para c√°lculos muy grandes
sbatch slurm_multi_node.sh
```

## üìä Estrategias de Paralelizaci√≥n

### Nivel 1: Paralelizaci√≥n por Bases
- Cada nodo procesa diferentes bases GTH
- Ideal para barrido de bases
- Ejemplo: `slurm_array_job.sh`

### Nivel 2: Paralelizaci√≥n por Puntos de Energ√≠a
- M√∫ltiples puntos de k-mesh o cutoff en paralelo
- Implementado en `incremental_pipeline.py`

### Nivel 3: Paralelizaci√≥n Interna de PySCF
- Paralelizaci√≥n autom√°tica por k-points
- Configurada v√≠a `OMP_NUM_THREADS`

## üîç Monitoreo y Depuraci√≥n

### Comandos √ötiles de SLURM

```bash
# Ver colas disponibles
sinfo

# Ver trabajos en cola
squeue -p gpu

# Ver detalles de un trabajo
scontrol show job <job_id>

# Cancelar trabajo
scancel <job_id>
```

### Logs y Debugging

```bash
# Ver logs en tiempo real
tail -f preconvergencia_out/preconv.log

# Ver m√©tricas de rendimiento
sacct -j <job_id> --format=JobID,JobName,Elapsed,CPUTime,MaxRSS

# Depurar problemas de memoria
sacct -j <job_id> --format=JobID,MaxRSS,MaxVMSize
```

## üìà Optimizaci√≥n de Rendimiento

### Configuraci√≥n por Tipo de Trabajo

| Tipo de C√°lculo | CPUs por Tarea | Memoria (GB) | Tiempo Estimado |
|-----------------|----------------|--------------|-----------------|
| Preconvergencia r√°pida | 4 | 16 | 2-4 horas |
| Optimizaci√≥n completa | 8 | 32 | 8-24 horas |
| Barrido de bases | 8 | 64 | 24-48 horas |
| C√°lculos de producci√≥n | 16 | 128 | 48-72 horas |

### Estrategias de Optimizaci√≥n

1. **Gesti√≥n de Memoria**:
   ```bash
   # Ajustar seg√∫n recursos disponibles
   export PYSCF_MAX_MEMORY=$((SLURM_MEM_PER_NODE * 1000 * 8 / 10))  # 80% de RAM disponible
   ```

2. **Paralelizaci√≥n Inteligente**:
   ```bash
   # Para trabajos intensivos en CPU
   export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

   # Para trabajos con muchos k-points
   export OMP_NUM_THREADS=1  # Dejar que PySCF maneje la paralelizaci√≥n
   ```

3. **Uso de Scratch**:
   ```bash
   # Usar almacenamiento local r√°pido
   WORKDIR=/scratch/$USER/job_$SLURM_JOB_ID
   mkdir -p $WORKDIR
   cd $WORKDIR
   ```

## ‚úÖ Validaci√≥n de Reproducibilidad

### Verificaci√≥n Autom√°tica

```bash
# Ejecutar validador despu√©s de cada c√°lculo
python reproducibility_validator.py

# Verificar integridad de archivos
python reproducibility_validator.py --output-file validation_report.json
```

### M√©tricas de Validaci√≥n

- **Fingerprint del entorno**: Identificador √∫nico del setup
- **Convergencia de cutoff**: ŒîE < 1 meV entre puntos
- **Convergencia de k-mesh**: ŒîE < 0.1 meV entre mallas
- **Par√°metro de red**: 5.5-5.8 √Ö (rango f√≠sico)
- **Gap electr√≥nico**: 1.0-2.0 eV (GaAs t√≠pico)

## üîÑ Flujos de Trabajo Recomendados

### Para Desarrollo Local
```bash
# Validaci√≥n r√°pida
python preconvergencia_GaAs.py --fast --timeout_s 300

# Con validaci√≥n de reproducibilidad
python preconvergencia_GaAs.py --fast --make_report on
python reproducibility_validator.py
```

### Para Producci√≥n en HPC
```bash
# Construir contenedor
singularity build preconvergencia-gaas.sif Singularity.def

# Ejecutar trabajo optimizado
sbatch slurm_job.sh

# Validar resultados
python reproducibility_validator.py
```

### Para Estudios Param√©tricos
```bash
# Barrido sistem√°tico
sbatch slurm_array_job.sh

# An√°lisis de resultados
python diagnostics.py
```

## üö® Soluci√≥n de Problemas

### Problemas Comunes

1. **Tiempo de espera agotado**:
   - Aumentar `--timeout_s`
   - Verificar recursos de SLURM
   - Considerar partici√≥n m√°s r√°pida

2. **Error de memoria**:
   - Reducir `PYSCF_MAX_MEMORY`
   - Aumentar `SLURM_MEM`
   - Usar menos procesos en paralelo

3. **Problemas de convergencia**:
   - Revisar par√°metros iniciales
   - Ajustar `sigma_ha`
   - Verificar estructura cristalina

4. **Problemas de Singularity**:
   - Verificar versi√≥n de Singularity
   - Comprobar permisos de archivos
   - Revisar bind mounts

### Logs de Diagn√≥stico

```bash
# Ver logs detallados
tail -f preconvergencia_out/preconv.log

# Ver m√©tricas de SLURM
sacct -j $SLURM_JOB_ID --format=JobID,State,ExitCode,Elapsed,CPUTime,MaxRSS

# Depurar contenedor
singularity shell preconvergencia-gaas.sif
```

## üìö Referencias y Recursos

- [Documentaci√≥n SLURM](https://slurm.schedmd.com/documentation.html)
- [Documentaci√≥n Singularity](https://sylabs.io/docs/)
- [PySCF Documentation](https://pyscf.org/)
- [Gu√≠a de Optimizaci√≥n](./optimization_guide.md)

## ü§ù Soporte

Para problemas espec√≠ficos del cluster:
1. Consultar documentaci√≥n local del HPC
2. Contactar administrador del sistema
3. Revisar logs detallados del trabajo
4. Usar herramientas de diagn√≥stico incluidas

---

**Nota**: Esta gu√≠a est√° optimizada para clusters con SLURM y Singularity. Adaptar seg√∫n la configuraci√≥n espec√≠fica del supercomputo utilizado.